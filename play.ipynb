{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの作成（学習は別途必要）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install einops-exts\n",
    "!pip install vit-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pytorch.vit import ViT\n",
    "from vit_pytorch.extractor import Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT(\n",
    "    image_size = 256,\n",
    "    patch_size = 32,\n",
    "    num_classes = 1000,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "vit = Extractor(vit, return_embeddings_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from flamingo_pytorch import FlamingoPaLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flamingo_palm = FlamingoPaLM(\n",
    "    num_tokens = 20000,          # number of tokens\n",
    "    dim = 1024,                  # dimensions\n",
    "    depth = 12,                  # depth\n",
    "    heads = 8,                   # attention heads\n",
    "    dim_head = 64,               # dimension per attention head\n",
    "    img_encoder f= vit,           # plugin your image encoder (this can be optional if you pass in the image embeddings separately, but probably want to train end to end given the perceiver resampler)\n",
    "    media_token_id = 3,          # the token id representing the [media] or [image]\n",
    "    cross_attn_every = 3,        # how often to cross attend\n",
    "    perceiver_num_latents = 64,  # perceiver number of latents, should be smaller than the sequence length of the image tokens\n",
    "    perceiver_depth = 2          # perceiver resampler depth\n",
    ")\n",
    "\n",
    "text = torch.randint(0, 20000, (2, 512))\n",
    "\n",
    "palm_logits = flamingo_palm(text)\n",
    "\n",
    "# after much training off the regular PaLM logits\n",
    "# now you are ready to train Flamingo + PaLM\n",
    "# by passing in images, it automatically freezes everything but the perceiver and cross attention blocks, as in the paper\n",
    "\n",
    "dialogue = torch.randint(0, 20000, (4, 512))\n",
    "images = torch.randn(4, 2, 3, 256, 256)\n",
    "\n",
    "flamingo_logits = flamingo_palm(text=dialogue, images=images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 20000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flamingo_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
